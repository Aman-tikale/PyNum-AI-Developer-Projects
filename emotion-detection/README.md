# ğŸ˜„ Emotion Detection using LSTM (PyTorch NLP)

A natural language processing project using PyTorch and LSTM (Long Short-Term Memory) networks to detect emotions from text data such as tweets or messages.

---

## ğŸ“Œ Project Overview

Emotion detection is a text classification task that identifies the emotional tone behind a body of text. This project uses an LSTM-based recurrent neural network to classify user messages into emotions such as joy, anger, sadness, fear, etc.

---

## ğŸ“ Project Structure

emotion-detection-lstm/
â”‚
â”œâ”€â”€ data/ # Contains emotion dataset (CSV or TXT)
â”œâ”€â”€ models/ # Trained model weights (.pt files)
â”œâ”€â”€ outputs/ # Evaluation results, predictions
â”œâ”€â”€ train.py # Script to train the model
â”œâ”€â”€ evaluate.py # Script to evaluate model performance
â”œâ”€â”€ preprocess.py # Text cleaning and tokenization
â”œâ”€â”€ utils.py # Utility functions
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md # Project documentation

---

## ğŸ“Š Dataset

- **Source**: [Emotion Dataset for NLP](https://www.kaggle.com/datasets/praveengovi/emotions-dataset-for-nlp)
- **Format**: Text + Emotion Label (CSV format)
- **Examples**:

| Text                          | Emotion   |
|------------------------------|-----------|
| "I feel fantastic!"          | joy       |
| "This is so frustrating."    | anger     |
| "Iâ€™m feeling really down."   | sadness   |

---

## ğŸ§  Model Architecture

- **Type**: LSTM-based Recurrent Neural Network
- **Embedding Layer**: Pretrained GloVe (optional)
- **LSTM Layers**: 1-2 stacked layers
- **Dropout**: Used for regularization
- **Output Layer**: Softmax over emotion classes
- **Loss**: CrossEntropyLoss
- **Optimizer**: Adam
- **Metrics**: Accuracy, F1-score

---

## âš™ï¸ How to Run the Project

### 1. Clone the Repository

```bash
git clone https://github.com/amantikale/emotion-detection-lstm.git
cd emotion-detection-lstm

pip install -r requirements.txt

python train.py

python evaluate.py

Validation Accuracy: 92.4%
Test Accuracy: 91.3%
F1 Score: 0.91

